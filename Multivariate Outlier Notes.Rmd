---
title: "Multivariate outlier notes"
author: "Katie Lotterhos"
date: "May 24, 2016"
output: html_document
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)
```

### Maronna 1976
* Don't understand

### Alfons 2016 multivariate association between two datasets.pdf
* The R Package ccaPP
* Alfons A, Croux C, Filzmoser P (2016) Robust Maximum Association Between Data Sets: The R Package ccaPP. Austrian J Stat 45:71
* An intuitive measure of association between two multivariate data sets can be defined as the maximal value that a bivariate association measure between any one-dimensional projections of each data set can attain
* Note that using the Pearson correlation as the projection index of the maximum associa- tion estimator corresponds to the first step of canonical correlation analysis (CCA; see, e.g., Johnson and Wichern 2002), hence the package name ccaPP. 
* We thereby use the classic diabetes data (Andrews and Herzberg 1985, page 215), which are included as example data in the package. Component x consists of p = 2 variables measuring relative weight and fasting plasma glucose, while component y consists of q = 3 variables measuring glucose intolerance, insulin response to oral glucose and insulin resistance. It is of medical interest to establish a relation between the two data sets.

### Biometrika-2015-Ro-589-99.pdf: Outlier detection for high-dimensional data
* Ro K, Zou C, Wang Z, Yin G (2015) Outlier detection for high-dimensional data. Biometrika 102:589–599
* For high- dimensional data, classical methods based on the Mahalanobis distance are usually not applicable. We propose an outlier detection procedure that replaces the classical minimum covariance determinant estimator with a high-breakdown minimum diagonal product estimator. 
* In robust statistics, estimation of the multivariate location parameter μ and covariance matrix is challenging, as many **classical methods break down in the presence of n/(p + 1) outliers**. (p is the number of variables or dimensions and n is the sample size)
* Even in the p < n case, as p increases, the traditional methods for outlier detection based on the Mahalanobis distance may become degenerate, and the contamination bias, which grows rapidly with p, could make the minimum covariance determinant unreliable for large p
* To overcome the difficulties with high-dimensional data, we modify the Mahalanobis distance so that it involves only the diagonal elements of the covariance matrix. The modified Mahalanobis distance (2) is invariant under a group of scalar transformations. Based on (2), we propose a high-breakdown minimum diagonal product estimator and develop an algorithm and threshold rule for outlier identification.
* minimum covariance determinant approach aims to find a subset of observations whose sample covariance matrix has the smallest determinant; this method, however, may not be reliable or well-defined for high-dimensional data. 
* Our method searches for a subset of h observations such that the product of the diagonal elements of the sample covariance matrix is minimal, and involves only the p marginal variances.

### Wang 2015 Multivariate outlier review for medical imaging
* Robust statistics perform well with data drawn from a wide range of probability distributions, especially for distributions that are not normally distributed. Robust statistical methods have been developed for many common problems, such as estimating data properties including location and scatter or estimating model param‐ eters as in regression analysis [10, 11]. One motivation is to produce statistical methods that are not unduly affected by outliers. Another motivation is to provide methods with good performance when there are small departures from a parametric distribution. A typical procedure or example of the former case is for multivariate estimation of location and covariance as well as for multivariate outlier detection. In this case, as a first step, the ap‐ proaches often try to search for a minimum number of observations with a certain degree of confidence being outlier-free. Based on this starting subset, location and covariance can be estimated robustly. In a second step, outliers can be identified through computing the observations’ distances with respect to these initial estimates.
* The **Minimum Covariance Determinant** (MCD) estimator is a highly robust estimator of multivariate location and scatter. 
    * dataset consisting of p variables and n observations, n x p matrix
    * The MCD relies on a subset of the total observations. Choosing this subset makes the algorithm robust because it is less sensitive to the influence of outlying points. 
    * MCD has a user-determined parameter, h , which specifies the size of the subset of data to base the estimate upon. It is constrained by (n + p + 1) / 2 ≤ h ≤ n. The h observations are chosen such that the determinant of the sample covariance matrix is minimal (but not mini‐ mized in the formal sense, because it relies on a sampling algorithm instead of a loss function)
    * MCD is most robust when h = (n + p + 1) / 2 
    * Computing the exact MCD is possible but computationally difficult, as it requires the evalu‐ ation of all ( hn ) subsets of size h .
    * so-called Fast-MCD algorithm uses an algorithm that concentrates on the h observations with the smallest distances and det (Σ^ MCD ,2) is more
concentrated (or equivalently, has a smaller determinant). 
    
* multivariate **Voronoi** outlier detection (MVOD) method for time series data
    * A Voronoi diagram is a way of dividing space into regions. 
    * The Voronoi Outlier Index (VOInd) used in our Multivariate Voronoi Outlier Detection (MVOD) method is based upon the Voronoi notion of nearest neighbors. For a point pi of set S , the nearest neighbors of pi defined by the Voronoi polygon V ( pi ) are the Voronoi nearest neighbor of pi, denoted as VNN (pi).


* **machine learning**
    * Novelty detection can be considered as the task of classifying test data that differ in some respect from the data that are available during training.
    * Novelty detection methods can be categorized into several areas such as probabilistic, distance-based, reconstruction-based, domain-based, and information-theoretic techniques.
    * **Probabilistic**
        * More complex data distribution forms can be modeled through mixture models (e.g. Gaussian Mixture Models, or GMMs for short), or other mixtures of different types of distributions
        * A common non-parametric approach for probabilistic density estimation is the kernel density estimator
        * The kernel density estimator places a kernel (e.g. Gaussian) on each data point and then sums the contributions from a localized neighborhood of the kernel. 
    * **Distance-based** approaches, such as clustering or nearest-neighbor methods [35-37], are another types of techniques that can be used for classification or for estimating the probability density function of data.     
    * **Reconstruction-based** methods involve training a regression model with the training data [3, 38, 39]. The distance between the test vector and the output of the system (i.e. the reconstruction error) can be related to the novelty score, which would be high when “abnormal” data occurs. For instance, neural networks can be used in this way
    * 

### rrcov: An Object-oriented Framework for Robust Multivariate Analysis
* Todorov V, Filzmoser P (2009) An Object-Oriented Framework for Robust Multivariate Analysis. J Stat Softw 32:1–47
* the Mahalanobis distance which is based on location and scatter estimates of the data set. obust estimates of these parameters are called for, even more, they must possess a positive breakdown point. The estimates of the multivariate location vector μ and the scatter matrix $\sum$ are also a cornerstone in the analysis of multidimensional data, since they form the input to many classical multivariate methods.
* Substituting the classical location and scatter estimates by their robust analogues is the most straightforward method for robustifying many multivariate procedures like principal components, discriminant and cluster analysis, canonical correlation, etc. 
*  These requirements have been defined in the project “Robust Statistics and R”, see http://www.statistik.tuwien.ac.at/rsr/, and a first step in this direction was the initial development of the collaborative package robustbase (Rousseeuw et al. 2009) with the intention that it becomes the essential robust statistics R package covering the methods described in the recent book Maronna et al. (2006).
* The corresponding robust methods can be seen as extensions to the classical ones which can cope with deviations from the stochastic assumptions thus mitigating the dangers for classical estimators. 
* The **minimum covariance determinant MCD** estimator for a data set is defined by that subset of h observations whose covariance matrix has the smallest determinant among all possible subsets of size h.
    * The MCD location and scatter estimate TMCD and CMCD are then given as the arithmetic mean and a multiple of the sample covariance matrix of that subset
    * The multiplication factors cccf (consistency correction factor) and csscf (small sample cor- rection factor) are selected so that C is consistent at the multivariate normal model and unbiased at small samples. A recommendable choice for h is b(n + p + 1)/2c because then the BP of the MCD is maximized, but any integer h within the interval [(n + p + 1)/2, n] can be chosen,
    * The naive algorithm would proceed by exhaustively investigating all subsets of size h out of n to find the subset with the smallest determinant of its covariance matrix, but this will be feasible only for very small data sets. The algorithm is based on the C-step, “C” in C-step stands for “concentration” since we are looking for a more “concentrated” covariance matrix with lower determinant.  the iteration process given by the C-step converges in a finite number of steps to a (local) minimum. Since there is no guarantee that the global minimum of the MCD objective function will be reached, the iteration must be started many times from different initial subsets, to obtain an approximate solution.
*  Also cover many other robust estimates
```{r}

install.packages(rrcov)
library(rrcov)

data("delivery")
delivery.x <- delivery[, 1:2]
data("hbk")
hbk.x <- hbk[, 1:3]
mcd <- CovMcd(delivery.x)
mcd
summary(mcd)
plot(mcd, which = "dd")

data("milk")
usr <- par(mfrow = c(1, 2))
plot(CovMcd(delivery[, 1:2]), which = "tolEllipsePlot", classic = TRUE)
plot(CovMcd(milk), which = "screeplot", classic = TRUE)
par(usr)
```

* PCA can also be robust.

### Filzmoser 2013 id local multivariate outliers.pdf

### Filzmoser outlier detection high dimensions.pdf

### FastPCS.pdf

#### Vakili fastPCS.pdf

#### Schmitt fast PCS.pdf

### Campbell Mahalanobis pathways humans.pdf

### Cerioli_2010.pdf

### Chen 2016 categorical clustering.pdf

### Codrea Mahalonobis mutant screening.pdf

### Daszykowski robust stats review.pdf

### De Maesschalck 2000 Mahalanobis chemistry.pdf

### Duong ks.pdf

### Farber Mahalanobis example species distribution.pdf


### Gao_etal_2005.pdf

### Ghosh_2013.pdf

### Gnanadesikan outlier.pdf

### Hun Oh Kernel-based approach outliers biology.pdf

### Jackson_et_al-2004-Environmetrics.pdf

### Kenward_et_al-2001-Ecology.pdf

### ks kernel package.pdf

### Mahalanobis.pdf

### Munoz outlier detection.pdf

### mvoutlier.pdf


### Shieh_2009.pdf

### Stapanian multivariate outliers fisheries kurtosis.pdf

### Todorov robust outlier.pdf

### Vakalli and Schmitt Fast PCS.pdf



### whitlock 2005.pdf

### Williams Mahalanobis example.pdf